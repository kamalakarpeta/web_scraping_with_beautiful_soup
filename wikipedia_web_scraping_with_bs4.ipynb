{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2b9393d",
   "metadata": {},
   "source": [
    "Let me provide you with a step-by-step guide on how to perform web scraping using Beautiful Soup in a Jupyter notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb90989",
   "metadata": {},
   "source": [
    "1. Import Necessary Libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca53f091",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8de72a5",
   "metadata": {},
   "source": [
    "2. Choose a URL to scrape from. For this example, we will scrape Wikipedia's list of 'The World's Billionaires'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8fc395c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://en.wikipedia.org/wiki/The_World%27s_Billionaires'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5d129f",
   "metadata": {},
   "source": [
    "3. Send a HTTP request to the specified URL and save the response from server in a response object called r."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d435f7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e68dfa",
   "metadata": {},
   "source": [
    "4. Create a Beautiful Soup object and specify the parser library at the same time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "561a04c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(r.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ed3711",
   "metadata": {},
   "source": [
    "5. Now we have to find the table and the rows within the table. We will use the find_all method of the soup object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee68b10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "table = soup.find('table', {'class': 'wikitable sortable'})\n",
    "rows = table.find_all('tr')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f9a6bd",
   "metadata": {},
   "source": [
    "6. Now we iterate through the rows to get the data for each billionaire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f89b1615",
   "metadata": {},
   "outputs": [],
   "source": [
    "billionaires = []\n",
    "for row in rows[1:]:  # the first row is the header row, we don't want that\n",
    "    cols = row.find_all('td')\n",
    "    billionaires.append({\n",
    "        'rank': cols[0].text.strip(),\n",
    "        'name': cols[1].text.strip(),\n",
    "        'net_worth': cols[2].text.strip(),\n",
    "        'age': cols[3].text.strip(),\n",
    "        'citizenship': cols[4].text.strip(),\n",
    "        'source': cols[5].text.strip()\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ee7174",
   "metadata": {},
   "source": [
    "7. Convert the list of billionaires into a pandas DataFrame for easier manipulation and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c9abe27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(billionaires)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e097ba5",
   "metadata": {},
   "source": [
    "8. Now you can perform any analysis you want on this data. For example, you can find the average age of billionaires, or the country with the most billionaires, etc.\n",
    "Please note that this is a basic example of web scraping. Depending on the complexity and structure of the website you're scraping, you may need to adjust your code accordingly. Also, always make sure to check the website's robots.txt file (e.g.,https://www.somesite.com/robots.txt) and respect the rules outlined there. It's also good practice to not overwhelm a website with rapid, repeated requests, which can be seen as a denial of service attack.\n",
    "\n",
    "Now, let's continue with the analysis part of the code:\n",
    "\n",
    "9. To find the average age of the billionaires:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "83ae863e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average age of the world's billionaires is 71.70\n"
     ]
    }
   ],
   "source": [
    "df['age'] = df['age'].astype(int)  # Convert age to integer\n",
    "average_age = df['age'].mean()\n",
    "print(f\"The average age of the world's billionaires is {average_age:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c3df6e",
   "metadata": {},
   "source": [
    "10. To find the country with the most billionaires:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4266b988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The country with the most billionaires is United States\n"
     ]
    }
   ],
   "source": [
    "country_counts = df['citizenship'].value_counts()\n",
    "most_common_country = country_counts.idxmax()\n",
    "print(f\"The country with the most billionaires is {most_common_country}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91373db7",
   "metadata": {},
   "source": [
    "11. To find the most common source of wealth:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "56459bcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most common source of wealth is Microsoft\n"
     ]
    }
   ],
   "source": [
    "source_counts = df['source'].value_counts()\n",
    "most_common_source = source_counts.idxmax()\n",
    "print(f\"The most common source of wealth is {most_common_source}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e52eb36",
   "metadata": {},
   "source": [
    "Remember that this is a basic example of web scraping and analysis. Depending on your specific needs, you might need to modify the code. For instance, you might want to handle missing data, outliers, or convert currency to a common standard. Also, keep in mind that the structure of websites can change, so a script that works today might need adjustments in the future."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
